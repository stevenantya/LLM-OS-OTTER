{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from  langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "import streamlit as st\n",
    "import pymupdf\n",
    "\n",
    "import logging\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('partition_pdf.log'),  # write logs to partition_pdf.log file\n",
    "        logging.StreamHandler()                    # also print logs to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "model = init_chat_model(\"o3-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = \"MCP9808\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasheet URL: https://cdn-shop.adafruit.com/datasheets/MCP9808.pdf\n",
      "Downloading datasheet...\n",
      "Datasheet already exists in the database.\n",
      "Loading datasheet...\n",
      "Datasheet loaded!\n"
     ]
    }
   ],
   "source": [
    "# Find and download sensor datasheet\n",
    "\n",
    "search_query = f\"{sensor_name} datasheet filetype:pdf\"\n",
    "search_results = DDGS().text(search_query)\n",
    "if search_results:\n",
    "    datasheet_url = search_results[0]['href']\n",
    "    print(f\"Datasheet URL: {datasheet_url}\")\n",
    "    print(\"Downloading datasheet...\")\n",
    "    response = requests.get(datasheet_url)\n",
    "    if response.status_code == 200:\n",
    "        if not os.path.exists(f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\"):     \n",
    "            with open(f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\", \"wb\") as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            print(\"Datasheet downloaded!\")\n",
    "        else:\n",
    "            print(\"Datasheet already exists in the database.\")\n",
    "    print(\"Loading datasheet...\")\n",
    "    datasheet_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\"\n",
    "    print(\"Datasheet loaded!\")\n",
    "else:\n",
    "    print(\"No datasheet found for this I2C sensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasheet partition exists. Loaded from local file\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "# Load and partition the datasheet into elements\n",
    "# 5 levels of partitioning\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "md_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/MD_DB/md_{sensor_name}.md\"\n",
    "if not os.path.exists(md_path):\n",
    "    md_text = pymupdf4llm.to_markdown(datasheet_path)\n",
    "    pathlib.Path(md_path).write_bytes(md_text.encode())\n",
    "    print(\"Datasheet Partition does not exist. Created a new parition\")\n",
    "else:\n",
    "    md_text = pathlib.Path(md_path).read_text()\n",
    "    print(\"Datasheet partition exists. Loaded from local file\")\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "docs = splitter.create_documents([md_text])\n",
    "\n",
    "print(len(docs))\n",
    "# Join all document contents into one string\n",
    "all_text = \"\\n\\n---------XXXX----------\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Save to a single file\n",
    "output_file = f\"/home/steven/FYP/v2_LLM_OS/LLM/MD_DB/split_md_{sensor_name}.md\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB found, loaded from local file\n"
     ]
    }
   ],
   "source": [
    "# Embed the datasheet chunks using FAISS\n",
    "#TODO: We might want to use multiple datasheets for the same sensor\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "vector_db_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_Vector_DB/{sensor_name}\"\n",
    "if not os.path.exists(vector_db_path):\n",
    "    vector_db = FAISS.from_documents(docs, embeddings)\n",
    "    vector_db.save_local(vector_db_path)\n",
    "    print(\"Vector DB not found, created and saved a new Vector DB\")\n",
    "else:\n",
    "    vector_db = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vector DB found, loaded from local file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 10 most similar chunks from the vector DB using cosine simlarity.\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "query = \"Sensor data output length in bytes\"\n",
    "\n",
    "retrieved_chunk = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunk 1: lower bytes, the upper byte must be right-shifted by\n",
      "4 bits (or multiply by 2 [4] ) and the lower byte must be leftshifted by 4 bits (or multiply by 2 [-4] ). Adding the results\n",
      "of the shifted values provides the temperature data in\n",
      "decimal format (see Equation 5-1).\n",
      "no\n",
      "NO. Chunk not helpful, moving to next chunk\n",
      "Retrieved Chunk 2: Shutdown\n",
      "\n",
      "Critical Trip Lock\n",
      "\n",
      "Alarm Window Lock\n",
      "\n",
      "Clear Alert\n",
      "\n",
      "Alert Status\n",
      "\n",
      "Output Control\n",
      "\n",
      "Critical Alert only\n",
      "\n",
      "Alert Polarity\n",
      "\n",
      "Alert Comp./Int.\n",
      "\n",
      "Configuration\n",
      "\n",
      "Tem p erature\n",
      "\n",
      "T UPPER Limit\n",
      "\n",
      "T LOWER Limit\n",
      "\n",
      "T CRITICAL Limit\n",
      "\n",
      "Manufacturer ID\n",
      "\n",
      "Device ID/Rev\n",
      "\n",
      "Resolution\n",
      "\n",
      "SMBus/Standard I [2] C™\n",
      "Interface\n",
      "\n",
      "\n",
      "Band Gap\n",
      "Temperature\n",
      "Sensor\n",
      "\n",
      "ΔΣ ADC\n",
      "\n",
      "+0.5°C\n",
      "+0.25°C\n",
      "+0.125°C\n",
      "+0.0625°C\n",
      "\n",
      "\n",
      "A0 A1 A2 Alert SDA SCL V DD GND\n",
      "\n",
      "DS25095A-page 2 © 2011 Microchip Technology Inc.\n",
      "\n",
      "\n",
      "-----\n",
      "no\n",
      "NO. Chunk not helpful, moving to next chunk\n",
      "Retrieved Chunk 3: |0x06|MSB|0|0|0|0|0|0|0|0|\n",
      "||LSB|0|1|0|1|0|1|0|0|\n",
      "|0x07|MSB|0|0|0|0|0|1|0|0|\n",
      "||LSB|0|0|0|0|0|0|0|0|\n",
      "|0x08|LSB|0|0|0|0|0|0|1|1|\n",
      "no\n",
      "NO. Chunk not helpful, moving to next chunk\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the chunks. Ask the LLM if the chunk is helpful for answering the query. (Chunk validation)\n",
    "# How do I ask LLM if the chunk is helpful, if not mark the chunk as not helpful and retrieve the next chunk?\n",
    "validation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant that validates if a provided document chunk is helpful in answering the user's query.\n",
    "\n",
    "    QUERY:\n",
    "    {query}\n",
    "\n",
    "    CHUNK:\n",
    "    {chunk}\n",
    "\n",
    "    Is this chunk helpful for answering the query? Respond ONLY with 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "validated_chunks = []\n",
    "\n",
    "# Inspect the retrieved chunks (optional, for debugging purposes)\n",
    "for idx, chunk in enumerate(retrieved_chunk):\n",
    "    print(f\"Retrieved Chunk {idx+1}: {chunk.page_content}\")\n",
    "    prompt = validation_prompt.format_messages(query=query, chunk=chunk.page_content)\n",
    "    # print(prompt)\n",
    "    response = model.invoke(prompt).content.strip().lower()\n",
    "    print(response)\n",
    "    if 'yes' in response:\n",
    "        validated_chunks.append(chunk)\n",
    "        print(\"YES. Chunk is helpful, proceeding with the next steps\")\n",
    "    else:\n",
    "        print(\"NO. Chunk not helpful, moving to next chunk\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Chunks: \n"
     ]
    }
   ],
   "source": [
    "# Consolidate the validated chunks\n",
    "consolidated_chunks = \"\"\n",
    "i = 1\n",
    "for chunk in validated_chunks:\n",
    "    consolidated_chunks += f\"{i}. {chunk.page_content}\\n\"\n",
    "    i += 1\n",
    "    \n",
    "print(f\"Consolidated Chunks: {consolidated_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The MCP9808 outputs its temperature data as a 16‑bit value, which means the raw data length is 2 bytes.\n",
      "\n",
      "To explain briefly without revealing all internal processing details:\n",
      "• I recalled that the MCP9808 sensor’s temperature register is formatted as a 16‑bit register.\n",
      "• Converting 16 bits to bytes gives 2 bytes.\n",
      "• Therefore, reading the temperature register returns 2 bytes of data.\n",
      "\n",
      "So, the raw data output length is 2 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought Reasoning LLM to extract the I2C address from the consolidated chunks\n",
    "# https://www.datacamp.com/tutorial/chain-of-thought-prompting\n",
    "prompt_i2c_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and an expert in I2C sensors.\n",
    "\n",
    "    Raw context:\n",
    "    {chunk}\n",
    "\n",
    "    From your knowledge, {sensor_name} sensor data output length in bytes? Show me the reasoning process step by step and use your memory.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt_i2c = prompt_i2c_template.format_messages(\n",
    "    chunk=consolidated_chunks,\n",
    "    sensor_name=sensor_name\n",
    ")\n",
    "\n",
    "CoT_response = model.invoke(prompt_i2c).content.strip()\n",
    "print(f\"Response: {CoT_response}\")\n",
    "\n",
    "# The context is correct. The output is wrong, but in chatgpt website, the output is correct.\n",
    "# Maybe they are using reasoning and chain of thought which might be super helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: the sensor data output length is 2 bytes\n"
     ]
    }
   ],
   "source": [
    "prompt_i2c_feedback_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and an expert in I2C Sensors.\n",
    "\n",
    "    My expert told me:\n",
    "    {i2c_CoT_response}\n",
    "\n",
    "    What are the {sensor_name} sensor data output length in bytes?\n",
    "    ONLY fill in this sentence, the sensor data output length is X bytes\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt_i2c_feedback = prompt_i2c_feedback_template.format_messages(\n",
    "    i2c_CoT_response=response,\n",
    "    sensor_name=sensor_name\n",
    ")\n",
    "i2c_feedback_response = model.invoke(prompt_i2c_feedback).content.strip()\n",
    "print(f\"Response: {i2c_feedback_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The numerical byte value extracted from the text is 2.\n"
     ]
    }
   ],
   "source": [
    "prompt_i2c_cleanup_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and values extractor.\n",
    "\n",
    "    My expert told me:\n",
    "    {i2c_feedback_response}\n",
    "\n",
    "    Extract only the numerical byte value.\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_i2c_cleanup = prompt_i2c_cleanup_template.format_messages(\n",
    "    i2c_feedback_response=i2c_feedback_response\n",
    ")\n",
    "i2c_cleanup_response = model.invoke(prompt_i2c_cleanup).content.strip()\n",
    "print(f\"Response: {i2c_cleanup_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
