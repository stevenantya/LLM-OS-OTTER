{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from  langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from duckduckgo_search import DDGS\n",
    "import requests\n",
    "import streamlit as st\n",
    "import pymupdf\n",
    "\n",
    "import logging\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('partition_pdf.log'),  # write logs to partition_pdf.log file\n",
    "        logging.StreamHandler()                    # also print logs to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "model = init_chat_model(\"o3-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = \"MCP9808\"\n",
    "size = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasheet URL: https://cdn-shop.adafruit.com/datasheets/MCP9808.pdf\n",
      "Downloading datasheet...\n",
      "Datasheet already exists in the database.\n",
      "Loading datasheet...\n",
      "Datasheet loaded!\n"
     ]
    }
   ],
   "source": [
    "# Find and download sensor datasheet\n",
    "\n",
    "search_query = f\"{sensor_name} datasheet filetype:pdf\"\n",
    "search_results = DDGS().text(search_query)\n",
    "if search_results:\n",
    "    datasheet_url = search_results[0]['href']\n",
    "    print(f\"Datasheet URL: {datasheet_url}\")\n",
    "    print(\"Downloading datasheet...\")\n",
    "    response = requests.get(datasheet_url)\n",
    "    if response.status_code == 200:\n",
    "        if not os.path.exists(f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\"):     \n",
    "            with open(f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\", \"wb\") as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            print(\"Datasheet downloaded!\")\n",
    "        else:\n",
    "            print(\"Datasheet already exists in the database.\")\n",
    "    print(\"Loading datasheet...\")\n",
    "    datasheet_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_DB/{sensor_name}.pdf\"\n",
    "    print(\"Datasheet loaded!\")\n",
    "else:\n",
    "    print(\"No datasheet found for this I2C sensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasheet partition exists. Loaded from local file\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "# Load and partition the datasheet into elements\n",
    "# 5 levels of partitioning\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "md_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/MD_DB/md_{sensor_name}.md\"\n",
    "if not os.path.exists(md_path):\n",
    "    md_text = pymupdf4llm.to_markdown(datasheet_path)\n",
    "    pathlib.Path(md_path).write_bytes(md_text.encode())\n",
    "    print(\"Datasheet Partition does not exist. Created a new parition\")\n",
    "else:\n",
    "    md_text = pathlib.Path(md_path).read_text()\n",
    "    print(\"Datasheet partition exists. Loaded from local file\")\n",
    "\n",
    "splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "docs = splitter.create_documents([md_text])\n",
    "\n",
    "print(len(docs))\n",
    "# Join all document contents into one string\n",
    "all_text = \"\\n\\n---------XXXX----------\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Save to a single file\n",
    "output_file = f\"/home/steven/FYP/v2_LLM_OS/LLM/MD_DB/split_md_{sensor_name}.md\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB found, loaded from local file\n"
     ]
    }
   ],
   "source": [
    "# Embed the datasheet chunks using FAISS\n",
    "#TODO: We might want to use multiple datasheets for the same sensor\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "vector_db_path = f\"/home/steven/FYP/v2_LLM_OS/LLM/Datasheet_Vector_DB/{sensor_name}\"\n",
    "if not os.path.exists(vector_db_path):\n",
    "    vector_db = FAISS.from_documents(docs, embeddings)\n",
    "    vector_db.save_local(vector_db_path)\n",
    "    print(\"Vector DB not found, created and saved a new Vector DB\")\n",
    "else:\n",
    "    vector_db = FAISS.load_local(vector_db_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vector DB found, loaded from local file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 10 most similar chunks from the vector DB using cosine simlarity.\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "query = \"sensor measurement data\"\n",
    "retrieved_chunk = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunk 1: |SENSOR SERIAL INTERFACE TIMING SPECIFICATIONS|Col2|Col3|Col4|Col5|Col6|\n",
      "|---|---|---|---|---|---|\n",
      "|Electrical Specifications: Unless otherwise indicated, V = 2.7V to 5.5V, T = -40°C to +125°C, GND = Ground DD A and C = 80 pF. (Note 1) L||||||\n",
      "|Parameters|Sym|Min|Max|Units|Conditions|\n",
      "|2-Wire SMBus/Standard Mode I2C™ Compatible Interface (Note 1)||||||\n",
      "|Serial Port Clock Frequency|f SC|0|400|kHz|(Note 2, 4)|\n",
      "|Low Clock|t LOW|1300|—|ns|(Note 2)|\n",
      "|High Clock|t HIGH|600|—|ns|(Note 2)|\n",
      "no\n",
      "NO. Chunk not helpful, moving to next chunk\n",
      "Retrieved Chunk 2: This sensor has an industry standard 400 kHz, 2-wire,\n",
      "SMBus/I [2] C compatible serial interface, allowing up to\n",
      "eight or sixteen sensors to be controlled with a single\n",
      "serial bus (see Table 3-2 for available Address codes).\n",
      "These features make the MCP9808 ideal for\n",
      "\n",
      "sophisticated, multi-zone, temperature-monitoring\n",
      "applications. **Packa g e T yp es**\n",
      "\n",
      "\n",
      "**8-Pin 2x3 DFN***\n",
      "\n",
      "SDA 1 8 V DD\n",
      "\n",
      "SCL 2 EP 7 A0\n",
      "\n",
      "9\n",
      "\n",
      "Alert 3 6 A1\n",
      "\n",
      "GND 4 5 A2\n",
      "\n",
      "\n",
      "**8-Pin MSOP**\n",
      "\n",
      "\n",
      "V DD\n",
      "\n",
      "A0\n",
      "\n",
      "A1\n",
      "\n",
      "A2\n",
      "\n",
      "\n",
      "**40%**\n",
      "\n",
      "**30%**\n",
      "no\n",
      "NO. Chunk not helpful, moving to next chunk\n",
      "Retrieved Chunk 3: - Food Processing\n",
      "\n",
      "- Personal Computers and Servers\n",
      "\n",
      "- PC Peripherals\n",
      "\n",
      "- Consumer Electronics\n",
      "\n",
      "- Handheld/Portable Devices **Tem p erature Accurac y**\n",
      "\n",
      "##### **Description**\n",
      "\n",
      "Microchip Technology Inc.’s MCP9808 digital\n",
      "temperature sensor converts temperatures between\n",
      "-20°C and +100°C to a digital word with\n",
      "±0.25°C/±0.5°C (typical/maximum) accuracy.\n",
      "yes\n",
      "YES. Chunk is helpful, proceeding with the next steps\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the chunks. Ask the LLM if the chunk is helpful for answering the query. (Chunk validation)\n",
    "# How do I ask LLM if the chunk is helpful, if not mark the chunk as not helpful and retrieve the next chunk?\n",
    "validation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an assistant that validates if a provided document chunk is helpful in answering the user's query.\n",
    "\n",
    "    QUERY:\n",
    "    {query}\n",
    "\n",
    "    CHUNK:\n",
    "    {chunk}\n",
    "\n",
    "    Is this chunk helpful for answering the query? Respond ONLY with 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "validated_chunks = []\n",
    "\n",
    "# Inspect the retrieved chunks (optional, for debugging purposes)\n",
    "for idx, chunk in enumerate(retrieved_chunk):\n",
    "    print(f\"Retrieved Chunk {idx+1}: {chunk.page_content}\")\n",
    "    prompt = validation_prompt.format_messages(query=query, chunk=chunk.page_content)\n",
    "    # print(prompt)\n",
    "    response = model.invoke(prompt).content.strip().lower()\n",
    "    print(response)\n",
    "    if 'yes' in response:\n",
    "        validated_chunks.append(chunk)\n",
    "        print(\"YES. Chunk is helpful, proceeding with the next steps\")\n",
    "    else:\n",
    "        print(\"NO. Chunk not helpful, moving to next chunk\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Chunks: 1. - Food Processing\n",
      "\n",
      "- Personal Computers and Servers\n",
      "\n",
      "- PC Peripherals\n",
      "\n",
      "- Consumer Electronics\n",
      "\n",
      "- Handheld/Portable Devices **Tem p erature Accurac y**\n",
      "\n",
      "##### **Description**\n",
      "\n",
      "Microchip Technology Inc.’s MCP9808 digital\n",
      "temperature sensor converts temperatures between\n",
      "-20°C and +100°C to a digital word with\n",
      "±0.25°C/±0.5°C (typical/maximum) accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consolidate the validated chunks\n",
    "consolidated_chunks = \"\"\n",
    "i = 1\n",
    "for chunk in validated_chunks:\n",
    "    consolidated_chunks += f\"{i}. {chunk.page_content}\\n\"\n",
    "    i += 1\n",
    "    \n",
    "print(f\"Consolidated Chunks: {consolidated_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Below is the answer with a step‐by‐step explanation:\n",
      "\n",
      "1. The MCP9808 sensor measures temperature.\n",
      "\n",
      "2. When you receive the two-byte (16‑bit) big‑endian raw data from the sensor, the digital temperature value is encoded across 12 bits. In the 16‑bit word, the temperature information occupies bits 15 down to 4. In the requested format, you would express that as:  \n",
      "  Temperature[15:4]\n",
      "\n",
      "Step‐by‐step reasoning:\n",
      "• From the provided context we know the sensor is a digital temperature sensor designed to measure temperature between –20°C and +100°C.\n",
      "\n",
      "• Internal documentation (and the sensor’s datasheet) tells us that the temperature is represented with a resolution of 0.0625°C, which implies that the value is stored as a scaled 12‑bit two’s complement number within a 16‑bit register. The two bytes that come from the sensor (in big‑endian order) are combined to form this 16‑bit word.\n",
      "\n",
      "• The sensor uses the upper 12 bits (bit positions 15 down to 4) to encode temperature, while the lower 4 bits are not part of the temperature value. (Details such as alert flags or configuration bits are not included, and the assignment only considers raw temperature data.)\n",
      "\n",
      "Thus, combining the two bytes into one parameter, we conclude that the only physical quantity represented in the raw data is temperature, which is encoded from bit 15 to bit 4.\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought Reasoning LLM to extract the I2C address from the consolidated chunks\n",
    "# https://www.datacamp.com/tutorial/chain-of-thought-prompting\n",
    "prompt_i2c_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and an expert in I2C Sensors. Assume ideal and default condition.\n",
    "\n",
    "    Raw context (might be inaccurate):\n",
    "    {chunk}\n",
    "\n",
    "    1. What physical quantities or parameters does the {sensor_name} measure?\n",
    "    2. Assuming raw_bytes is a {size}-byte array received from the sensor, provide the bit range for each physical parameter in the format: ParameterName[start_bit:end_bit] (For example, Temperature[0:12])\n",
    "    3. The raw_bytes is big-endian.\n",
    "    4. Omit anything that is unrelated to the raw data such as alert, config, or crc.\n",
    "    5. Do not describe MSB/LSB or byte-level structure, rather combine them into one parameter.\n",
    "    6. Please explain your reasoning step by step, using both the context and your internal knowledge.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt_i2c = prompt_i2c_template.format_messages(\n",
    "    chunk=consolidated_chunks,\n",
    "    sensor_name=sensor_name,\n",
    "    size=size,\n",
    ")\n",
    "\n",
    "CoT_response = model.invoke(prompt_i2c).content.strip()\n",
    "print(f\"Response: {CoT_response}\")\n",
    "\n",
    "# The context is correct. The output is wrong, but in chatgpt website, the output is correct.\n",
    "# Maybe they are using reasoning and chain of thought which might be super helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: the raw measurement values are arranged as: (temperature: [15:4])\n"
     ]
    }
   ],
   "source": [
    "prompt_i2c_feedback_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and an expert in I2C Sensors.\n",
    "\n",
    "    My expert told me:\n",
    "    {i2c_CoT_response}\n",
    "\n",
    "    How is the raw measurement values arranged in {sensor_name}? Extract only the measurement parameters.\n",
    "    ONLY FILL IN the sentence, the raw measurement values are arranged as: (parameter1: [index1:index1], parameter2: [index2:index2], ...)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompt_i2c_feedback = prompt_i2c_feedback_template.format_messages(\n",
    "    i2c_CoT_response=CoT_response,\n",
    "    sensor_name=sensor_name\n",
    ")\n",
    "i2c_feedback_response = model.invoke(prompt_i2c_feedback).content.strip()\n",
    "print(f\"Response: {i2c_feedback_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: the raw measurement values are arranged as: (temperature: [15:4])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "prompt_i2c_cleanup_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant and an expert in I2C Sensors.\n",
    "\n",
    "    My expert told me:\n",
    "    {i2c_feedback_response}\n",
    "\n",
    "    Convert the arrangement to the correct format.\n",
    "    If the value spans multiple bytes, only use the first and the last index.\n",
    "    ONLY FILL IN the sentence, the raw measurement values are arranged as: (parameter1: [index1:index1], parameter2: [index2:index2], ...)\n",
    "    \"\"\"\n",
    ")\n",
    "prompt_i2c_cleanup = prompt_i2c_cleanup_template.format_messages(\n",
    "    i2c_feedback_response=i2c_feedback_response\n",
    ")\n",
    "i2c_cleanup_response = model.invoke(prompt_i2c_cleanup).content.strip()\n",
    "print(f\"Response: {i2c_cleanup_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: temperature: [15:4]\n"
     ]
    }
   ],
   "source": [
    "# Extract only content within parentheses\n",
    "matches = re.findall(r'\\((.*?)\\)', i2c_cleanup_response)\n",
    "\n",
    "extracted_content = matches[0] if matches else \"\"\n",
    "\n",
    "print(f\"Response: {extracted_content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
